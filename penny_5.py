import pennylane as qml
import numpy as np
import torch
from torch import nn
import scipy.io as sio
import matplotlib.pyplot as plt
from torch.utils.data import DataLoader, TensorDataset
import time

# Global parameters
ENVIR = 'indoor'
LAYERS = 2
BATCHES = 5
CHANNELS = 2

# image parameters
IMG_HEIGHT = 32
IMG_WIDTH = IMG_HEIGHT
IMG_DIM = IMG_HEIGHT * IMG_WIDTH * CHANNELS
IMG_QUBITS = int(np.log2(IMG_DIM))

# compressed parameters
COM_HEIGHT = 16
COM_WIDTH = COM_HEIGHT
COM_DIM = COM_HEIGHT * COM_WIDTH * CHANNELS
COM_QUBITS = int(np.log2(COM_DIM))

ALL_QUBITS = IMG_QUBITS + 1
ANC_QUBITS = IMG_QUBITS - COM_QUBITS


class ClassicalNN(nn.Module):
    ''' æ„é€ ç»å…¸å‹ç¼©ç¥ç»ç½‘ç»œ '''
    def __init__(self, channels, img_height, com_height):
        super().__init__()
        self.img_dim = channels * img_height**2
        self.com_dim = channels * com_height**2
        self.conv = nn.Conv2d(in_channels=channels, out_channels=2, kernel_size=3, stride=1, padding=1, bias=True)
        self.leaky_relu = nn.LeakyReLU(negative_slope=0.2)
        self.dense_encode = nn.Linear(in_features=self.img_dim, out_features=self.com_dim)
        self.bn1d = nn.BatchNorm1d(num_features=channels)

    def forward(self, x):
        ''' å®šä¹‰ç»å…¸å‹ç¼©å±‚ '''
        x = self.conv(x)
        x = self.leaky_relu(x)
        x = x.reshape((x.shape[0], -1))  # å±•å¹³
        x = self.dense_encode(x)
        x = 2 * self.bn1d(x)  # ç¼©æ”¾å‚æ•°èŒƒå›´åˆ°[-2, 2]
        return x


def frqi_encoder(qubits, params, target_wire=0):
    ''' æ”¹è¿›çš„FRQIç¼–ç ç”µè·¯ '''
    # å¯¹æ•°æ®é‡å­æ¯”ç‰¹åº”ç”¨Hadamardé—¨åˆ›å»ºå åŠ æ€
    for i in range(1, qubits + 1):
        qml.Hadamard(wires=i)
    # ä½¿ç”¨å—æ§æ—‹è½¬è¿›è¡Œç¼–ç 
    for index in range(min(2**qubits, len(params))):
        binary_str = bin(index)[2:].zfill(qubits)
        bits = [int(bit) for bit in binary_str]
        bits.reverse()
        qml.ctrl(qml.RY, control=range(1, qubits + 1), control_values=bits)(params[index], wires=target_wire)


coe = [-1]
obs_list = [qml.PauliZ(0)]
hamiltonian = qml.Hamiltonian(coe, observables=obs_list)

dev = qml.device('default.qubit', wires=ALL_QUBITS)


@qml.qnode(dev, interface='torch')
def quantum_circuit(com_params, img_params, quantum_params):
    ''' é‡å­ç”µè·¯ - ä½¿ç”¨å‚æ•°ç§»ä½è§„åˆ™è®¡ç®—æ¢¯åº¦ '''
    # ç¼–ç å‹ç¼©å‚æ•°
    frqi_encoder(COM_QUBITS, com_params, target_wire=0)
    # å¼ºçº ç¼ å±‚
    qml.StronglyEntanglingLayers(weights=quantum_params, wires=range(ALL_QUBITS))
    # ç¼–ç åŸå§‹å›¾åƒå‚æ•°
    frqi_encoder(IMG_QUBITS, img_params, target_wire=0)
    return qml.expval(hamiltonian)


class HybridNN(nn.Module):
    def __init__(self, classical_nn, com_qubits, img_qubits):
        super().__init__()
        self.classical_nn = classical_nn
        self.com_qubits = com_qubits
        self.img_qubits = img_qubits
        self.all_qubits = img_qubits + 1
        # é‡å­å‚æ•° - ä½¿ç”¨numpyæ•°ç»„å­˜å‚¨ï¼Œä¸æ³¨å†Œä¸ºnn.Parameter
        self.quantum_params = np.random.uniform(0, np.pi, size=(LAYERS, self.all_qubits, 3))

    def forward(self, x):
        ''' å‰å‘ä¼ æ’­ '''
        # ç»å…¸ç¥ç»ç½‘ç»œå‰å‘ä¼ æ’­
        com_params = self.classical_nn(x)  # [batch_size, com_dim]
        # å°†è¾“å…¥æ•°æ®å±•å¹³
        batch_size = x.shape[0]
        x_flat = x.reshape(batch_size, -1)  # [batch_size, img_dim]
        # å¯¹æ¯ä¸ªæ ·æœ¬è¿è¡Œé‡å­ç”µè·¯
        energies = []
        for i in range(batch_size):
            energy = quantum_circuit(
                com_params[i].detach().numpy(),  # ç»å…¸å‚æ•°ï¼Œä¸éœ€è¦é‡å­æ¢¯åº¦
                x_flat[i].detach().numpy(),      # è¾“å…¥æ•°æ®ï¼Œä¸éœ€è¦é‡å­æ¢¯åº¦
                self.quantum_params              # é‡å­å‚æ•°ï¼Œéœ€è¦å‚æ•°ç§»ä½è§„åˆ™
            )
            energies.append(energy)
        return torch.stack(energies)
    def get_quantum_params(self):
        """è·å–é‡å­å‚æ•°"""
        return self.quantum_params.copy()
    def set_quantum_params(self, new_params):
        """è®¾ç½®é‡å­å‚æ•°"""
        self.quantum_params = new_params.copy()


class QuantumGradientOptimizer:
    """ä½¿ç”¨å‚æ•°ç§»ä½è§„åˆ™ä¼˜åŒ–é‡å­å‚æ•°çš„ä¼˜åŒ–å™¨"""
    def __init__(self, quantum_circuit, learning_rate=0.1):
        self.quantum_circuit = quantum_circuit
        self.lr = learning_rate
    def compute_gradient(self, com_params, img_params, quantum_params, shift=np.pi/2):
        """ä½¿ç”¨å‚æ•°ç§»ä½è§„åˆ™è®¡ç®—æ¢¯åº¦"""
        gradient = np.zeros_like(quantum_params)
        # å¯¹æ¯ä¸ªå‚æ•°è®¡ç®—æ¢¯åº¦
        for layer in range(quantum_params.shape[0]):
            for qubit in range(quantum_params.shape[1]):
                for param_idx in range(quantum_params.shape[2]):
                    # å‚æ•°ç§»ä½è§„åˆ™ï¼šf(Î¸+Ï€/2) - f(Î¸-Ï€/2)
                    params_plus = quantum_params.copy()
                    params_plus[layer, qubit, param_idx] += shift
                    
                    params_minus = quantum_params.copy()
                    params_minus[layer, qubit, param_idx] -= shift
                    
                    # è®¡ç®—ä¸¤ä¸ªç‚¹çš„æœŸæœ›å€¼
                    f_plus = self.quantum_circuit(com_params, img_params, params_plus)
                    f_minus = self.quantum_circuit(com_params, img_params, params_minus)
                    
                    # è®¡ç®—æ¢¯åº¦
                    gradient[layer, qubit, param_idx] = (f_plus - f_minus) / 2
        
        return gradient
    
    def update_params(self, com_params, img_params, quantum_params):
        """æ›´æ–°é‡å­å‚æ•°"""
        gradient = self.compute_gradient(com_params, img_params, quantum_params)
        new_params = quantum_params - self.lr * gradient
        return new_params


def train_hybrid_nn():
    ''' è®­ç»ƒæ··åˆç¥ç»ç½‘ç»œ '''
    # åˆå§‹åŒ–æ¨¡å‹
    classical_nn = ClassicalNN(channels=CHANNELS, img_height=IMG_HEIGHT, com_height=COM_HEIGHT)
    hybrid_nn = HybridNN(classical_nn=classical_nn, com_qubits=COM_QUBITS, img_qubits=IMG_QUBITS)
    
    # åˆå§‹åŒ–ä¼˜åŒ–å™¨
    quantum_optimizer = QuantumGradientOptimizer(quantum_circuit, learning_rate=0.1)
    classical_optimizer = torch.optim.Adam(classical_nn.parameters(), lr=0.001)
    
    # æŸå¤±å‡½æ•°
    criterion = nn.MSELoss()
    
    # æ•°æ®å‡†å¤‡
    x_train_tensor = torch.tensor(x_train)
    x_val_tensor = torch.tensor(x_val)
    x_test_tensor = torch.tensor(x_test)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_dataset = TensorDataset(x_train_tensor, torch.zeros(len(x_train_tensor)))
    train_loader = DataLoader(train_dataset, batch_size=BATCHES, shuffle=True)
    
    # è®­ç»ƒå‚æ•°
    num_epochs = 50
    train_losses = []
    val_losses = []
    quantum_param_history = []
    
    print("å¼€å§‹è®­ç»ƒæ··åˆé‡å­-ç»å…¸ç¥ç»ç½‘ç»œ...")
    print("ğŸ“Š è®­ç»ƒç­–ç•¥:")
    print("  - ç»å…¸å‚æ•°: ä½¿ç”¨åå‘ä¼ æ’­(BP)")
    print("  - é‡å­å‚æ•°: ä½¿ç”¨å‚æ•°ç§»ä½è§„åˆ™(Parameter-shift Rule)")
    print(f"è®­ç»ƒæ ·æœ¬æ•°: {len(x_train)}")
    print(f"æ‰¹æ¬¡å¤§å°: {BATCHES}")
    
    start_time = time.time()
    
    for epoch in range(num_epochs):
        # è®­ç»ƒé˜¶æ®µ
        classical_nn.train()
        epoch_train_loss = 0.0
        num_batches = 0
        
        for batch_idx, (data, target) in enumerate(train_loader):
            # é‡ç½®æ¢¯åº¦
            classical_optimizer.zero_grad()
            
            # å‰å‘ä¼ æ’­ - ç»å…¸éƒ¨åˆ†
            com_params = classical_nn(data)  # ç»å…¸å‹ç¼©
            
            # è·å–å½“å‰é‡å­å‚æ•°
            current_quantum_params = hybrid_nn.get_quantum_params()
            
            batch_size = data.shape[0]
            x_flat = data.reshape(batch_size, -1)
            
            # å¯¹æ‰¹æ¬¡ä¸­çš„æ¯ä¸ªæ ·æœ¬å•ç‹¬ä¼˜åŒ–é‡å­å‚æ•°
            batch_quantum_gradients = []
            batch_energies = []
            
            for i in range(batch_size):
                # ä½¿ç”¨å‚æ•°ç§»ä½è§„åˆ™æ›´æ–°é‡å­å‚æ•°
                new_quantum_params = quantum_optimizer.update_params(
                    com_params[i].detach().numpy(),
                    x_flat[i].detach().numpy(),
                    current_quantum_params
                )
                
                # è®¾ç½®æ–°çš„é‡å­å‚æ•°
                hybrid_nn.set_quantum_params(new_quantum_params)
                
                # è®¡ç®—å½“å‰æ ·æœ¬çš„èƒ½é‡ï¼ˆæŸå¤±ï¼‰
                energy = quantum_circuit(
                    com_params[i].detach().numpy(),
                    x_flat[i].detach().numpy(),
                    new_quantum_params
                )
                batch_energies.append(energy)
            
            # è®¡ç®—ç»å…¸éƒ¨åˆ†çš„æŸå¤±
            energies_tensor = torch.tensor(batch_energies, dtype=torch.float32)
            loss = criterion(energies_tensor, torch.zeros_like(energies_tensor))
            
            # ç»å…¸éƒ¨åˆ†çš„åå‘ä¼ æ’­
            loss.backward()
            classical_optimizer.step()
            
            epoch_train_loss += loss.item()
            num_batches += 1
            
            if batch_idx % 5 == 0:
                print(f'Epoch {epoch+1:03d} | Batch {batch_idx:03d} | Loss: {loss.item():.6f}')
        
        avg_train_loss = epoch_train_loss / num_batches if num_batches > 0 else 0
        train_losses.append(avg_train_loss)
        
        # éªŒè¯é˜¶æ®µ
        classical_nn.eval()
        with torch.no_grad():
            val_energies = []
            for i in range(len(x_val_tensor)):
                com_params_val = classical_nn(x_val_tensor[i:i+1])
                x_flat_val = x_val_tensor[i:i+1].reshape(1, -1)
                
                energy_val = quantum_circuit(
                    com_params_val[0].detach().numpy(),
                    x_flat_val[0].detach().numpy(),
                    hybrid_nn.get_quantum_params()
                )
                val_energies.append(energy_val)
            
            val_energies_tensor = torch.tensor(val_energies, dtype=torch.float32)
            val_loss = criterion(val_energies_tensor, torch.zeros_like(val_energies_tensor)).item()
            val_losses.append(val_loss)
        
        # ä¿å­˜é‡å­å‚æ•°å†å²
        quantum_param_history.append(hybrid_nn.get_quantum_params().copy())
        
        print(f'Epoch {epoch+1:03d}/{num_epochs:03d} | '
              f'è®­ç»ƒæŸå¤±: {avg_train_loss:.6f} | '
              f'éªŒè¯æŸå¤±: {val_loss:.6f}')
        
        # æ—©åœæ£€æŸ¥
        if epoch > 10 and val_loss > np.mean(val_losses[-5:]):
            print("éªŒè¯æŸå¤±ä¸Šå‡ï¼Œè€ƒè™‘æ—©åœ...")
            if epoch > 20:
                break
    
    training_time = time.time() - start_time
    print(f'è®­ç»ƒå®Œæˆ! æ€»æ—¶é—´: {training_time:.2f}ç§’')
    
    # ä¿å­˜æ¨¡å‹
    save_trained_model(classical_nn, hybrid_nn, quantum_param_history)
    
    # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    plot_training_curves(train_losses, val_losses, quantum_param_history)
    
    # æµ‹è¯•æ¨¡å‹
    test_model(classical_nn, hybrid_nn, x_test_tensor)
    
    return classical_nn, hybrid_nn, train_losses, val_losses


def save_trained_model(classical_nn, hybrid_nn, quantum_param_history):
    """ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹"""
    # ä¿å­˜ç»å…¸ç¥ç»ç½‘ç»œ
    torch.save(classical_nn.state_dict(), 'classical_nn.pth')
    
    # ä¿å­˜é‡å­å‚æ•°
    np.save('quantum_params.npy', hybrid_nn.get_quantum_params())
    np.save('quantum_param_history.npy', np.array(quantum_param_history))
    
    print("æ¨¡å‹å·²ä¿å­˜!")


def plot_training_curves(train_losses, val_losses, quantum_param_history):
    """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
    
    # æŸå¤±æ›²çº¿
    ax1.plot(train_losses, label='è®­ç»ƒæŸå¤±', alpha=0.7)
    ax1.plot(val_losses, label='éªŒè¯æŸå¤±', alpha=0.7)
    ax1.set_xlabel('è®­ç»ƒå‘¨æœŸ')
    ax1.set_ylabel('æŸå¤±å€¼')
    ax1.set_title('è®­ç»ƒè¿‡ç¨‹')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # é‡å­å‚æ•°å˜åŒ–
    quantum_params_flat = [params.flatten() for params in quantum_param_history]
    ax2.plot(quantum_params_flat, alpha=0.5)
    ax2.set_xlabel('è®­ç»ƒå‘¨æœŸ')
    ax2.set_ylabel('é‡å­å‚æ•°å€¼')
    ax2.set_title('é‡å­å‚æ•°å˜åŒ–')
    ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('training_results.png', dpi=300, bbox_inches='tight')
    plt.show()


def test_model(classical_nn, hybrid_nn, test_data):
    """æµ‹è¯•æ¨¡å‹"""
    classical_nn.eval()
    
    with torch.no_grad():
        test_energies = []
        for i in range(min(10, len(test_data))):  # æµ‹è¯•å‰10ä¸ªæ ·æœ¬
            com_params_test = classical_nn(test_data[i:i+1])
            x_flat_test = test_data[i:i+1].reshape(1, -1)
            
            energy_test = quantum_circuit(
                com_params_test[0].detach().numpy(),
                x_flat_test[0].detach().numpy(),
                hybrid_nn.get_quantum_params()
            )
            test_energies.append(energy_test)
        
        print(f"\næµ‹è¯•ç»“æœ:")
        print(f"èƒ½é‡èŒƒå›´: [{min(test_energies):.3f}, {max(test_energies):.3f}]")
        print(f"èƒ½é‡å‡å€¼: {np.mean(test_energies):.3f} Â± {np.std(test_energies):.3f}")


if __name__ == "__main__":
    # æ•°æ®åŠ è½½ï¼ˆä¿æŒåŸæœ‰ä»£ç ï¼‰
    if ENVIR == 'indoor':
        mat = sio.loadmat('../../DataSpace/csinet/data/DATA_Htrainin.mat')
        x_train = mat['HT']
        mat = sio.loadmat('../../DataSpace/csinet/data/DATA_Hvalin.mat')
        x_val = mat['HT']
        mat = sio.loadmat('../../DataSpace/csinet/data/DATA_Htestin.mat')
        x_test = mat['HT']
    else:
        mat = sio.loadmat('../../DataSpace/csinet/data/DATA_Htrainout.mat')
        x_train = mat['HT']
        mat = sio.loadmat('../../DataSpace/csinet/data/DATA_Hvalout.mat')
        x_val = mat['HT']
        mat = sio.loadmat('../../DataSpace/csinet/data/DATA_Htestout.mat')
        x_test = mat['HT']

    x_train = x_train.astype('float32')
    x_val = x_val.astype('float32')
    x_test = x_test.astype('float32')
    x_train = np.reshape(x_train, (len(x_train), CHANNELS, IMG_HEIGHT, IMG_WIDTH))
    x_val = np.reshape(x_val, (len(x_val), CHANNELS, IMG_HEIGHT, IMG_WIDTH))
    x_test = np.reshape(x_test, (len(x_test), CHANNELS, IMG_HEIGHT, IMG_WIDTH))

    print('æ•°æ®åŠ è½½å®Œæˆï¼Œå¼€å§‹è®­ç»ƒ...')
    
    # å¼€å§‹è®­ç»ƒ
    classical_nn_trained, hybrid_nn_trained, train_losses, val_losses = train_hybrid_nn()
