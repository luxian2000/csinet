import time
import pennylane as qml
import numpy as np
import torch
from torch import nn
import scipy.io as sio
from torch.utils.data import TensorDataset, DataLoader

# Global parameters
ENVIR = 'indoor'
LAYERS = 4
BATCHES = 5
CHANNELS = 2

# image parameters
IMG_HEIGHT = 32
IMG_WIDTH = IMG_HEIGHT
IMG_DIM = IMG_HEIGHT * IMG_WIDTH * CHANNELS
IMG_QUBITS = int(np.log2(IMG_DIM))

# compressed parameters
COM_HEIGHT = 8
COM_WIDTH = COM_HEIGHT 
COM_DIM = COM_HEIGHT * COM_WIDTH * CHANNELS
COM_QUBITS = int(np.log2(COM_DIM))

ALL_QUBITS = IMG_QUBITS + 1
ANC_QUBITS = IMG_QUBITS - COM_QUBITS

# Data loading
if ENVIR == 'indoor':
    mat = sio.loadmat('../../DataSpace/csinet/data/DATA_Htrainin.mat')
    x_train = mat['HT']  # array
    mat = sio.loadmat('../../DataSpace/csinet/data/DATA_Hvalin.mat')
    x_val = mat['HT']  # array
    mat = sio.loadmat('../../DataSpace/csinet/data/DATA_Htestin.mat')
    x_test = mat['HT']  # array

elif ENVIR == 'outdoor':
    mat = sio.loadmat('../../DataSpace/csinet/data/DATA_Htrainout.mat')
    x_train = mat['HT']  # array
    mat = sio.loadmat('../../DataSpace/csinet/data/DATA_Hvalout.mat')
    x_val = mat['HT']  # array
    mat = sio.loadmat('../../DataSpace/csinet/data/DATA_Htestout.mat')
    x_test = mat['HT']  # array

x_train = x_train.astype('float32')
x_val = x_val.astype('float32')
x_test = x_test.astype('float32')
print('x_train çš„åŸå§‹ç»´åº¦:', x_train.shape)

x_train = np.reshape(x_train, (len(x_train), CHANNELS, IMG_HEIGHT, IMG_WIDTH))
x_val = np.reshape(x_val, (len(x_val), CHANNELS, IMG_HEIGHT, IMG_WIDTH))
x_test = np.reshape(x_test, (len(x_test), CHANNELS, IMG_HEIGHT, IMG_WIDTH))
print('x_train çš„å¡‘å½¢ç»´åº¦:', x_train.shape)


class ClassicalNN(nn.Module):
    ''' æ„é€ ç»å…¸å‹ç¼©ç¥ç»ç½‘ç»œ '''
    def __init__(self):
        super().__init__()
        self.conv = nn.Conv2d(in_channels=CHANNELS, out_channels=2, kernel_size=3, stride=1, padding=1, bias=True)
        self.leaky_relu = nn.LeakyReLU(negative_slope=0.2)
        self.dense_encode = nn.Linear(in_features=IMG_DIM, out_features=COM_DIM)
        self.bn1d = nn.BatchNorm1d(num_features=1)

    def forward(self, x):
        ''' å®šä¹‰ç»å…¸å‹ç¼©å±‚ '''
        x = self.conv(x)
        x = self.leaky_relu(x)
        x = x.reshape((x.shape[0], -1))     # [batch, features]
        x = self.dense_encode(x)            # [batch, com_dim]
        x = x.unsqueeze(1)                 # [batch, 1, com_dim]
        x = self.bn1d(x)
        x = 2 * x                           # æ³¨æ„åœ¨è¿™é‡Œä¹˜ä»¥2
        return x


def frqi_encoder(qubits, params, target=0):
    ''' construct the FRQI encoding circuit '''
    for index in range(2**qubits):
        binary_str = bin(index)[2:].zfill(qubits)
        bits = [int(bit) for bit in binary_str]
        bits.reverse()  # åŸåœ°é€†åºï¼Œé«˜ä½åœ¨å³ï¼Œä½œç”¨äºåºæ•°å¤§çš„æ¯”ç‰¹ä½
        qml.ctrl(qml.RY, control=range(1, qubits + 1), control_values=bits)(params[index], wires=target)


coe = [-1]
obs_list = [qml.PauliZ(0)]
hamiltonian = qml.Hamiltonian(coe, observables=obs_list)

dev = qml.device('default.qubit', wires=ALL_QUBITS)


@qml.qnode(dev, interface='torch')
def frqi_circuit(com_params, img_params, asz_params):
    ''' construct the complete quantum circuit '''
    for i in range(1, COM_QUBITS + 1):
        qml.Hadamard(wires=i)

    frqi_encoder(COM_QUBITS, com_params)
    qml.StronglyEntanglingLayers(weights=asz_params, wires=range(ALL_QUBITS))
    frqi_encoder(IMG_QUBITS, img_params)

    for i in range(1, IMG_QUBITS + 1):
        qml.Hadamard(wires=i)

    return qml.expval(hamiltonian)


class HybridNN(nn.Module):
    ''' æŠŠä¸Šé¢å®šä¹‰çš„ç»å…¸ç¥ç»ç½‘ç»œå’Œé‡å­ç¥ç»ç½‘ç»œç»„è£…æˆå®Œæ•´ç¥ç»ç½‘ç»œ '''
    def __init__(self, classical_nn):
        super().__init__()
        self.classical_nn = classical_nn

        asz_params = np.random.uniform(0, np.pi, size=(LAYERS, ALL_QUBITS, 3))
        self.asz_params = nn.Parameter(torch.tensor(asz_params, dtype=torch.float32))

    def forward(self, x):
        ''' åœ¨é‡å­çº¿è·¯å‰ï¼ŒåŠ ä¸Šç»å…¸å‹ç¼©ç½‘ç»œ '''
        com_params = self.classical_nn(x)

        # å°†è¾“å…¥æ•°æ®å±•å¹³ç”¨äºé‡å­ç¼–ç 
        x = (-1) * x
        batch_size = x.shape[0]
        x_flat = x.reshape(batch_size, -1)

        # å¯¹batckä¸­çš„æ¯ä¸ªæ ·æœ¬å•ç‹¬å¤„ç†
        energies = []
        for i in range(batch_size):
            try:
                energy = frqi_circuit(com_params[i], x_flat[i], self.asz_params)
                energies.append(energy)
            except Exception as e:
                print(f"é‡å­çº¿è·¯æ‰§è¡Œé”™è¯¯ï¼š{e}")
                energies.append(torch.tensor(0.0))

        # loss = frqi_circuit(com_params, x, self.asz_params)
        return torch.stack(energies)


def train_hybrid_nn():
    ''' è®­ç»ƒæ··åˆç¥ç»ç½‘ç»œ '''
    # åˆå§‹åŒ–æ¨¡å‹
    classical_nn = ClassicalNN()
    hybrid_nn = HybridNN(classical_nn=classical_nn)

    # æ£€æŸ¥å¯è®­ç»ƒå‚æ•°
    print("å¯è®­ç»ƒå‚æ•°åˆ†æ")
    total_params = 0
    for name, param in hybrid_nn.named_parameters():
        if param.requires_grad:
            print(f"   {name}: {param.shape} ({param.numel()} ä¸ªå‚æ•°)")
            total_params += param.numel()
    print(f" æ€»å¯è®­ç»ƒå‚æ•°æ•°é‡: {total_params}")

    # ä¼˜åŒ–å™¨
    optimizer = torch.optim.Adam(hybrid_nn.parameters(), lr=0.001)

    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5)

    # æŸå¤±å‡½æ•° - ç›®æ ‡æ˜¯ä½¿é‡å­ç”µè·¯è¾“å‡ºæ¥è¿‘0ï¼ˆæˆ–å…¶ä»–ç›®æ ‡å€¼ï¼‰
    criterion = nn.MSELoss()
    target_value = 0.0  # å¯ä»¥æ ¹æ®ä»»åŠ¡è°ƒæ•´

    # æ•°æ®å‡†å¤‡
    x_train_tensor = torch.tensor(x_train)
    x_val_tensor = torch.tensor(x_val)
    x_test_tensor = torch.tensor(x_test)

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_dataset = TensorDataset(x_train_tensor, torch.zeros(len(x_train_tensor)))
    train_loader = DataLoader(train_dataset, batch_size=min(BATCHES, len(x_train)), shuffle=True)

    # è®­ç»ƒå‚æ•°
    num_epochs = 50
    best_val_loss = float('inf')
    train_losses = []
    val_losses = []
    quantum_param_norms = []  # è·Ÿè¸ªé‡å­å‚æ•°çš„å˜åŒ–

    print("å¼€å§‹è®­ç»ƒæ··åˆé‡å­-ç»å…¸ç¥ç»ç½‘ç»œ...")
    print(f" è®­ç»ƒæ ·æœ¬æ•°: {len(x_train)}")
    print(f" éªŒè¯æ ·æœ¬æ•°: {len(x_val)}")
    print(f" æ‰¹æ¬¡å¤§å°: {BATCHES}")
    print(f" ç›®æ ‡å€¼: {target_value}")
    print(f" é‡å­æ¯”ç‰¹æ•°: {ALL_QUBITS} (è¾…åŠ©: 1, æ•°æ®: {ALL_QUBITS-1})")

    start_time = time.time()

    for epoch in range(num_epochs):
        # === è®­ç»ƒé˜¶æ®µ ===
        hybrid_nn.train()
        epoch_train_loss = 0.0
        num_batches = 0

        for batch_idx, (data, target) in enumerate(train_loader):
            try:
                optimizer.zero_grad()

                # å‰å‘ä¼ æ’­
                outputs = hybrid_nn(data)

                # è®¡ç®—æŸå¤± - ä½¿é‡å­ç”µè·¯è¾“å‡ºæ¥è¿‘ç›®æ ‡å€¼
                loss = criterion(outputs, torch.full_like(outputs, target_value))

                # åå‘ä¼ æ’­
                loss.backward()

                # æ¢¯åº¦è£å‰ªï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
                torch.nn.utils.clip_grad_norm_(hybrid_nn.parameters(), max_norm=1.0)

                # å‚æ•°æ›´æ–°
                optimizer.step()

                epoch_train_loss += loss.item()
                num_batches += 1

                if batch_idx % 5 == 0:
                    print(f'ğŸ“ Epoch {epoch+1:03d} | Batch {batch_idx:03d} | Loss: {loss.item():.6f}')

            except Exception as e:
                print(f"âŒ è®­ç»ƒæ‰¹æ¬¡ {batch_idx} å‡ºé”™: {e}")
                continue


train_hybrid_nn()
